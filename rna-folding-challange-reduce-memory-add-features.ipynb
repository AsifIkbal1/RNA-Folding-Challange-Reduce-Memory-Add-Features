{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Stanford Ribonanza RNA Folding Challange\nPS: I am not an an expert, I am doing some chatgpt google research to possible solutions or baseline to such problem, my background in Physics but always loved biology too. \nMy main motivation to learn and provide simple starter code for people\n\n- Tasks: Reduce memory, Use BPP files, Add folding","metadata":{}},{"cell_type":"code","source":"import tqdm\nimport pandas as pd \nimport os \nfrom tqdm import tqdm as tq\nfrom collections import defaultdict\nimport numpy as np\nimport multiprocessing","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-10T13:42:19.996652Z","iopub.execute_input":"2023-09-10T13:42:19.997372Z","iopub.status.idle":"2023-09-10T13:42:20.48925Z","shell.execute_reply.started":"2023-09-10T13:42:19.997291Z","shell.execute_reply":"2023-09-10T13:42:20.487956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"let's read sample data","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/stanford-ribonanza-rna-folding/test_sequences.csv')[:50000]\ntrain = pd.read_csv('/kaggle/input/stanford-ribonanza-rna-folding/train_data.csv')[:250000]\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-10T13:42:20.491601Z","iopub.execute_input":"2023-09-10T13:42:20.492067Z","iopub.status.idle":"2023-09-10T13:44:25.883586Z","shell.execute_reply.started":"2023-09-10T13:42:20.492037Z","shell.execute_reply":"2023-09-10T13:44:25.882126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-10T13:44:25.884837Z","iopub.execute_input":"2023-09-10T13:44:25.886174Z","iopub.status.idle":"2023-09-10T13:44:25.934491Z","shell.execute_reply.started":"2023-09-10T13:44:25.886108Z","shell.execute_reply":"2023-09-10T13:44:25.932905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_dataframe_size(df):\n    # Make a copy of the DataFrame to avoid modifying the original\n    df_copy = df.copy()\n\n    # Define a mapping of column names to more memory-efficient data types\n    column_type_mapping = {}\n\n    for column in df_copy.columns:\n        dtype = df_copy[column].dtype\n\n        if dtype == 'object':\n            # Handle object columns (e.g., strings)\n            unique_values = df_copy[column].nunique()\n\n            if unique_values == 1:\n                # If a column has only one unique value, convert it to the appropriate data type\n                if 'int' in dtype.name:\n                    column_type_mapping[column] = 'int64'\n                elif 'float' in dtype.name:\n                    column_type_mapping[column] = 'float64'\n                elif 'datetime' in dtype.name:\n                    column_type_mapping[column] = 'datetime64'\n                else:\n                    column_type_mapping[column] = 'category'\n            elif unique_values < len(df_copy) / 2:\n                # If a column has less than half unique values, convert it to category\n                column_type_mapping[column] = 'category'\n        elif dtype == 'float64':\n            # Reduce float64 columns to float32 or float16\n            column_type_mapping[column] = 'float32'\n        elif dtype == 'int64':\n            # Reduce int64 columns to int32 or int16\n            column_type_mapping[column] = 'int32'\n        elif dtype == 'datetime64':\n            # Reduce datetime64 columns to datetime32\n            column_type_mapping[column] = 'datetime32'\n    \n    # Apply the data type conversions based on the mapping\n    df_copy = df_copy.astype(column_type_mapping)\n\n    return df_copy\nreduced_train = reduce_dataframe_size(train)","metadata":{"execution":{"iopub.status.busy":"2023-09-10T13:44:25.938183Z","iopub.execute_input":"2023-09-10T13:44:25.938577Z","iopub.status.idle":"2023-09-10T13:44:26.843896Z","shell.execute_reply.started":"2023-09-10T13:44:25.938546Z","shell.execute_reply":"2023-09-10T13:44:26.842752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-10T13:44:26.845433Z","iopub.execute_input":"2023-09-10T13:44:26.845781Z","iopub.status.idle":"2023-09-10T13:44:26.875618Z","shell.execute_reply.started":"2023-09-10T13:44:26.845752Z","shell.execute_reply":"2023-09-10T13:44:26.874078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduced_test = reduce_dataframe_size(test)\nreduced_test.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-10T13:44:26.877199Z","iopub.execute_input":"2023-09-10T13:44:26.877697Z","iopub.status.idle":"2023-09-10T13:44:26.972275Z","shell.execute_reply.started":"2023-09-10T13:44:26.87765Z","shell.execute_reply":"2023-09-10T13:44:26.971078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-10T13:44:26.973948Z","iopub.execute_input":"2023-09-10T13:44:26.97444Z","iopub.status.idle":"2023-09-10T13:44:27.017015Z","shell.execute_reply.started":"2023-09-10T13:44:26.974394Z","shell.execute_reply":"2023-09-10T13:44:27.01525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduced_train.info()","metadata":{"execution":{"iopub.status.busy":"2023-09-10T13:44:27.019739Z","iopub.execute_input":"2023-09-10T13:44:27.02016Z","iopub.status.idle":"2023-09-10T13:44:27.065561Z","shell.execute_reply.started":"2023-09-10T13:44:27.020106Z","shell.execute_reply":"2023-09-10T13:44:27.062954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduced_train","metadata":{"execution":{"iopub.status.busy":"2023-09-10T13:44:27.069393Z","iopub.execute_input":"2023-09-10T13:44:27.070047Z","iopub.status.idle":"2023-09-10T13:44:27.209084Z","shell.execute_reply.started":"2023-09-10T13:44:27.070008Z","shell.execute_reply":"2023-09-10T13:44:27.207684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Getting txt file as a single dataframe which is base-pairing probability matrix of each sequences in our train and test dataset. Definition:\n- **A base-pairing probability matrix** (BPPM) stores the probabilities for every possible base pair in an RNA sequence and has been used in many algorithms in RNA informatics (e.g., RNA secondary structure prediction and motif search). [source](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3514005/)\n","metadata":{}},{"cell_type":"code","source":"root_dir = '/kaggle/input/stanford-ribonanza-rna-folding/Ribonanza_bpp_files/extra_data'\nfile_paths = []\n\nfor folder, _, files in tq(os.walk(root_dir), total=len(os.listdir(root_dir))):\n    for file in files:\n        file_paths.append(os.path.join(folder, file))\n\n# Now, file_paths contains the list of all file paths within the specified directory and its subdirectories.\n","metadata":{"execution":{"iopub.status.busy":"2023-09-10T13:44:27.214057Z","iopub.execute_input":"2023-09-10T13:44:27.214478Z","iopub.status.idle":"2023-09-10T13:54:54.880373Z","shell.execute_reply.started":"2023-09-10T13:44:27.214446Z","shell.execute_reply":"2023-09-10T13:54:54.879055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- filter sequences only in dataset","metadata":{}},{"cell_type":"code","source":"# Get unique sequence_id values from both dataframes\nunique_sequence_ids = set(reduced_train['sequence_id']) | set(reduced_test['sequence_id'])\n\n# Initialize tqdm with the list of file paths\nwith tq(total=len(file_paths), desc=\"Filtering Files\") as pbar:\n    # Filter file paths using list comprehension\n    filtered_file_paths = [\n        filepath\n        for filepath in file_paths\n        if os.path.splitext(os.path.basename(filepath))[0] in unique_sequence_ids\n    ]\n    pbar.update(len(file_paths))  # Update the progress bar to completion","metadata":{"execution":{"iopub.status.busy":"2023-09-10T13:54:54.884478Z","iopub.execute_input":"2023-09-10T13:54:54.885414Z","iopub.status.idle":"2023-09-10T13:55:05.092191Z","shell.execute_reply.started":"2023-09-10T13:54:54.885368Z","shell.execute_reply":"2023-09-10T13:55:05.09086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can get avarage of probability for each sequence_id","metadata":{}},{"cell_type":"code","source":"# Create a defaultdict to store sequence_id and their corresponding probabilities\nsequence_probabilities = defaultdict(list)\n\n# Initialize tqdm with the list of text files\nwith tq(total=len(filtered_file_paths), desc=\"Processing Files\") as pbar:\n    # Iterate through each file path\n    for filepath in filtered_file_paths:\n        with open(filepath, \"r\") as file:\n            for line in file:\n                # Split each line into values\n                values = line.strip().split()  # Split based on spaces\n\n                # Ensure that there are at least 3 values (pair1, pair2, probability)\n                if len(values) < 3:\n                    continue\n\n                # Extract pair1, pair2, and probability\n                _, _, probability = values[:3]\n\n                # Use the filename (without extension) as the key\n                sequence_id = os.path.splitext(os.path.basename(filepath))[0]\n\n                # Append the probability to the list for the sequence_id\n                sequence_probabilities[sequence_id].append(float(probability))\n\n        pbar.update(1)  # Update the progress bar\n\n# Calculate the average probability for each sequence_id\naverage_probabilities = {}\nfor sequence_id, probabilities in sequence_probabilities.items():\n    average_prob = sum(probabilities) / len(probabilities)\n    average_probabilities[sequence_id] = average_prob","metadata":{"execution":{"iopub.status.busy":"2023-09-10T13:55:05.094186Z","iopub.execute_input":"2023-09-10T13:55:05.094628Z","iopub.status.idle":"2023-09-10T15:05:38.433033Z","shell.execute_reply.started":"2023-09-10T13:55:05.094593Z","shell.execute_reply":"2023-09-10T15:05:38.431715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"delete some memory","metadata":{}},{"cell_type":"code","source":"del train\ndel test\ndel file_paths\ndel filtered_file_paths","metadata":{"execution":{"iopub.status.busy":"2023-09-10T15:05:38.434996Z","iopub.execute_input":"2023-09-10T15:05:38.435369Z","iopub.status.idle":"2023-09-10T15:05:39.432704Z","shell.execute_reply.started":"2023-09-10T15:05:38.435337Z","shell.execute_reply":"2023-09-10T15:05:39.431391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Adding average probability to train and test dataset and drop null columns","metadata":{}},{"cell_type":"code","source":"avg_p = pd.DataFrame(average_probabilities, index=[0]).T.reset_index()\navg_p.columns = ['sequence_id', 'avg_probability']\nreduced_train_new  = reduced_train.merge(avg_p, on='sequence_id', how='left').dropna(axis=1,how='all')\nreduced_test_new  = reduced_test.merge(avg_p, on='sequence_id', how='left').dropna(axis=1,how='all')","metadata":{"execution":{"iopub.status.busy":"2023-09-10T15:05:39.434239Z","iopub.execute_input":"2023-09-10T15:05:39.434665Z","iopub.status.idle":"2023-09-10T15:05:52.240629Z","shell.execute_reply.started":"2023-09-10T15:05:39.434633Z","shell.execute_reply":"2023-09-10T15:05:52.239452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del reduced_train\ndel reduced_test\ndel average_probabilities\ndel avg_p","metadata":{"execution":{"iopub.status.busy":"2023-09-10T15:05:52.242705Z","iopub.execute_input":"2023-09-10T15:05:52.243181Z","iopub.status.idle":"2023-09-10T15:05:52.263382Z","shell.execute_reply.started":"2023-09-10T15:05:52.243109Z","shell.execute_reply":"2023-09-10T15:05:52.261994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduced_train_new","metadata":{"execution":{"iopub.status.busy":"2023-09-10T15:05:52.266534Z","iopub.execute_input":"2023-09-10T15:05:52.266902Z","iopub.status.idle":"2023-09-10T15:05:52.436197Z","shell.execute_reply.started":"2023-09-10T15:05:52.266872Z","shell.execute_reply":"2023-09-10T15:05:52.434814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduced_test_new","metadata":{"execution":{"iopub.status.busy":"2023-09-10T15:05:52.438577Z","iopub.execute_input":"2023-09-10T15:05:52.439569Z","iopub.status.idle":"2023-09-10T15:05:52.466886Z","shell.execute_reply.started":"2023-09-10T15:05:52.439527Z","shell.execute_reply":"2023-09-10T15:05:52.465431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Folding RNA structures, using swifter for faster apply.","metadata":{}},{"cell_type":"code","source":"!pip install viennarna\n!pip install swifter","metadata":{"execution":{"iopub.status.busy":"2023-09-10T15:05:52.469032Z","iopub.execute_input":"2023-09-10T15:05:52.470451Z","iopub.status.idle":"2023-09-10T15:06:30.407386Z","shell.execute_reply.started":"2023-09-10T15:05:52.47041Z","shell.execute_reply":"2023-09-10T15:06:30.406068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ViennaRNA import fold\nimport swifter\n\n\n# Apply complex_structure_features calculation in parallel with a progress bar\ndef process_with_progress(df):\n    with tq(total=len(df), desc=\"Processing Data\") as pbar:\n        df['complex_structure_features'] = df['sequence'].swifter.apply(fold)\n        pbar.update(len(df))  # Update the progress bar to completion\n\n# Apply the processing function to your dataframes\nprocess_with_progress(reduced_train_new)\nprocess_with_progress(reduced_test_new)","metadata":{"execution":{"iopub.status.busy":"2023-09-10T15:06:30.409679Z","iopub.execute_input":"2023-09-10T15:06:30.410012Z","iopub.status.idle":"2023-09-10T17:17:58.796397Z","shell.execute_reply.started":"2023-09-10T15:06:30.409982Z","shell.execute_reply":"2023-09-10T17:17:58.794143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to count parentheses\ndef count_parentheses(structure_string):\n    count = structure_string.count(\")\")\n    return count\n\n# Apply the function to the DataFrame column\ntq.pandas()\nreduced_train_new['parentheses_counts'] = reduced_train_new['complex_structure_features'].astype(str).apply(count_parentheses)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-10T17:17:58.799192Z","iopub.execute_input":"2023-09-10T17:17:58.801896Z","iopub.status.idle":"2023-09-10T17:17:59.968439Z","shell.execute_reply.started":"2023-09-10T17:17:58.801856Z","shell.execute_reply":"2023-09-10T17:17:59.967288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduced_train_new","metadata":{"execution":{"iopub.status.busy":"2023-09-10T17:17:59.970076Z","iopub.execute_input":"2023-09-10T17:17:59.970844Z","iopub.status.idle":"2023-09-10T17:18:00.232532Z","shell.execute_reply.started":"2023-09-10T17:17:59.970806Z","shell.execute_reply":"2023-09-10T17:18:00.231226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install forgi","metadata":{"execution":{"iopub.status.busy":"2023-09-10T17:18:00.234388Z","iopub.execute_input":"2023-09-10T17:18:00.234811Z","iopub.status.idle":"2023-09-10T17:18:22.786915Z","shell.execute_reply.started":"2023-09-10T17:18:00.234777Z","shell.execute_reply":"2023-09-10T17:18:22.785771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import forgi.visual.mplotlib as fvm\nimport forgi.graph.bulge_graph as fgb\nimport random\nimport matplotlib.pyplot as plt\n\n# Assuming you have a DataFrame called reduced_train_new with the structures and sequences\n\n# Number of random structures to plot\nnum_structures_to_plot = 5\n\n# Get random indices from your DataFrame\nrandom_indices = random.sample(range(len(reduced_train_new)), num_structures_to_plot)\n\n# Create subplots for each structure\nplt.figure(figsize=(15, 5 * num_structures_to_plot))\n\nfor i, idx in enumerate(random_indices, 1):\n    structure = reduced_train_new['complex_structure_features'][idx][0]\n    sequence = reduced_train_new['sequence'][idx]\n\n    plt.subplot(num_structures_to_plot, 1, i)\n    bg = fgb.BulgeGraph.from_fasta_text(f'>rna{idx}\\n{structure}\\n{sequence}')[0]\n    fvm.plot_rna(bg, lighten=0.5, text_kwargs={\"fontweight\": None})\n    plt.title(f\"RNA Structure {idx}\")\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-10T17:21:19.665432Z","iopub.execute_input":"2023-09-10T17:21:19.665834Z","iopub.status.idle":"2023-09-10T17:21:32.847786Z","shell.execute_reply.started":"2023-09-10T17:21:19.665803Z","shell.execute_reply":"2023-09-10T17:21:32.846691Z"},"trusted":true},"execution_count":null,"outputs":[]}]}